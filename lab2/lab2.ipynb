{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Homography estimation and applications\n",
    "\n",
    "In this lab you will learn how to estimate a homography relating two images given a set of correspondences between them. Then, you will work with different computer vision applications of the homography.\n",
    "\n",
    "More precisely, the goals are:\n",
    "\n",
    "1) Homography estimation with the DLT algorithm. <br>\n",
    "\n",
    "2) Application: Image mosaics. <br>\n",
    "\n",
    "3) Refinement of the estimated homography with the Gold Standard algorithm. <br>\n",
    "\n",
    "4) Application: Camera calibration with a planar pattern and augmented reality. <br>\n",
    "\n",
    "5) Application: Logo detection. <br>\n",
    "\n",
    "6) Application: Logo replacement.\n",
    "\n",
    "The following file combines some text cells (Markdown cells) and code cells. Some parts of the code need to be completed. All tasks you need to complete are marked in <span style='color:Green'> green.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter\n",
    "from utils import apply_H_fixed_image_size, plot_img, Ransac_DLT_homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Homography estimation with the DLT algorithm**\n",
    "\n",
    "### **1.1 Compute image correspondences**\n",
    "Execute the following code to find image correspondences using ORB [1] (you may also use SIFT, SURF, etc)\n",
    "\n",
    "[1] Ethan Rublee, Vincent Rabaud, Kurt Konolige, Gary R. Bradski: ORB: An efficient alternative to SIFT or SURF. ICCV 2011: 2564-2571."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "img1 = cv2.imread('Data/llanes/llanes_a.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('Data/llanes/llanes_b.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img3 = cv2.imread('Data/llanes/llanes_c.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "# find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "kp3, des3 = orb.detectAndCompute(img3,None)\n",
    "\n",
    "# Keypoint matching\n",
    "bf = cv2.BFMatcher()\n",
    "matches_12 = bf.knnMatch(des1,des2,k=2)\n",
    "# Apply ratio test\n",
    "good_matches_12 = []\n",
    "for m,n in matches_12:\n",
    "    if m.distance < 0.85*n.distance:\n",
    "        good_matches_12.append([m])\n",
    "\n",
    "matches_23 = bf.knnMatch(des2,des3,k=2)\n",
    "# Apply ratio test\n",
    "good_matches_23 = []\n",
    "for m,n in matches_23:\n",
    "    if m.distance < 0.85*n.distance:\n",
    "        good_matches_23.append([m])\n",
    "\n",
    "\n",
    "# Show \"good\" matches \n",
    "img_12 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "img_23 = cv2.drawMatchesKnn(img2,kp2,img3,kp3,good_matches_23,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_23)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Compute the homography (DLT algorithm) between image pairs**\n",
    "\n",
    "The following cell of code calls the 'Ransac_DLT_homography' function from utils.py\n",
    "Before running the code of the cell you first need to complete some functions in utils.py that are called from 'Ransac_DLT_homography' function:\n",
    "\n",
    "<span style='color:Green'> - Complete the 'DLT_homography' function that computes a homography from a set of point correspondences with the DLT algorithm.  </span>\n",
    "\n",
    "<span style='color:Green'> - Complete the 'Inliers' function that, given a homography, a set of point correspondences and a certain threshold  returns the indices of the correspondences that are inliers.  </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homography between images 1 and 2\n",
    "points1 = []\n",
    "points2 = []\n",
    "for m in good_matches_12:\n",
    "    points1.append([kp1[m[0].queryIdx].pt[0], kp1[m[0].queryIdx].pt[1], 1])\n",
    "    points2.append([kp2[m[0].trainIdx].pt[0], kp2[m[0].trainIdx].pt[1], 1])\n",
    "    \n",
    "points1 = np.asarray(points1)\n",
    "points1 = points1.T\n",
    "points2 = np.asarray(points2)\n",
    "points2 = points2.T\n",
    "\n",
    "H_12, indices_inlier_matches_12 = Ransac_DLT_homography(points1, points2, 3, 1000)\n",
    "inlier_matches_12 = itemgetter(*indices_inlier_matches_12)(good_matches_12)\n",
    "\n",
    "img_12 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,inlier_matches_12,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "# Homography between images 2 and 3\n",
    "points2 = []\n",
    "points3 = []\n",
    "for m in good_matches_23:\n",
    "    points2.append([kp2[m[0].queryIdx].pt[0], kp2[m[0].queryIdx].pt[1], 1])\n",
    "    points3.append([kp3[m[0].trainIdx].pt[0], kp3[m[0].trainIdx].pt[1], 1])\n",
    "    \n",
    "points2 = np.asarray(points2)\n",
    "points2 = points2.T\n",
    "points3 = np.asarray(points3)\n",
    "points3 = points3.T\n",
    "\n",
    "H_23, indices_inlier_matches_23 = Ransac_DLT_homography(points2, points3, 3, 1000)\n",
    "inlier_matches_23 = itemgetter(*indices_inlier_matches_23)(good_matches_23)\n",
    "\n",
    "img_23 = cv2.drawMatchesKnn(img2,kp2,img3,kp3,inlier_matches_23,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_23)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Application: Image mosaics**\n",
    "\n",
    "Now that the homographies relating the images have been estimated we can create a mosaic by properly transforming and fusing the different image contents.\n",
    "For that we create a big enough canvas where the mosaic will be created. The function 'apply_H_fixed_image_size' is provided in utils.py, it is a variant of the 'apply_H' function from lab 1 where this time the size of the transformed is fixed and given as an extra input.\n",
    "\n",
    "<span style='color:Green'> - Complete the 2nd input argument to the 'apply_H_fixed_image_size' function in order to get the image mosaic. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = [-400, 1200, -100, 650] # corners of the canvas where the mosaic will be created\n",
    "\n",
    "#print(img1.shape)\n",
    "img1c = cv2.imread('Data/llanes/llanes_a.jpg',cv2.IMREAD_COLOR)\n",
    "img2c = cv2.imread('Data/llanes/llanes_b.jpg',cv2.IMREAD_COLOR)\n",
    "img3c = cv2.imread('Data/llanes/llanes_c.jpg',cv2.IMREAD_COLOR)\n",
    "img1c = cv2.cvtColor(img1c, cv2.COLOR_BGR2RGB)\n",
    "img2c = cv2.cvtColor(img2c, cv2.COLOR_BGR2RGB)\n",
    "img3c = cv2.cvtColor(img3c, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img1c_w = apply_H_fixed_image_size(img1c, ??, corners) # complete the call to the function\n",
    "img2c_w = apply_H_fixed_image_size(img2c, ??, corners) # complete the call to the function\n",
    "img3c_w = apply_H_fixed_image_size(img3c, ??, corners) # complete the call to the function\n",
    "\n",
    "img_mosaic = np.maximum(img3c_w,np.maximum(img1c_w,img2c_w))\n",
    "plot_img(img_mosaic)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "img_mosaic = cv2.cvtColor(img_mosaic, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite('mosaic_llanes.png', img_mosaic) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style='color:Green'> - Compute the mosaic with castle_int images. </span>\n",
    "\n",
    "<span style='color:Green'> - Compute the mosaic with aerial images set 13 </span>\n",
    "\n",
    "<span style='color:Green'> - Compute the mosaic with aerial images set 22 </span>\n",
    "\n",
    "<span style='color:Green'> - In the report, comment the results in every of the four cases: hypothetise why it works or does not work </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Refinement of the estimated homography with the Gold Standard algorithm**\n",
    "\n",
    "In order to refine the previous estimated homography we can minimize the geometric error with the Levenberg-Marquardt algorithm. For that we will call the function 'least_squares' with the proper input arguments that you have to complete. First, it is recommended to read the documentation of that function:\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html\n",
    "\n",
    "Notice that the first input is a function that only provides the vector of the residuals. The algorithm constructs the cost function as a sum of squares of the residuals. In our case the function is called 'geometric_error_terms' and you need to complete it.\n",
    "\n",
    "More precisely, the tasks to perform are:\n",
    "\n",
    "<span style='color:Green'> - Complete the function 'geometric_error_terms'. </span>\n",
    "\n",
    "<span style='color:Green'> - Complete the code before calling function 'geometric_error_terms' in order to create the proper input of variables, 'variables0', to pass to the function. </span>\n",
    "\n",
    "<span style='color:Green'> - Extract the refined homogaphy and the refined keypoint locations from the output, 'result', of function 'geometric_error_terms'.  </span>\n",
    "\n",
    "<span style='color:Green'> - To better compare the two homographies (refined and non refined) compute and print the geometric error before and after the refinement. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares\n",
    "from math import ceil\n",
    "\n",
    "def geometric_error_terms(variables, data_points):\n",
    "    \n",
    "    # to complete ...\n",
    "    \n",
    "    return # to complete ...\n",
    "\n",
    "# create array variables0 ....\n",
    "\n",
    "result = least_squares(geometric_error_terms, variables0, method='lm', args=([points]))\n",
    "\n",
    "# from result extract refined homogaphy and the refined keypoint locations ...\n",
    "\n",
    "# print initial and final square errors ...\n",
    "\n",
    "\n",
    "# display points and refined points\n",
    "fig, ax = plt.subplots() # image 1\n",
    "ax.imshow(img1c)\n",
    "ax.scatter(points1[0,:],points1[1,:], c='cyan', marker='+')\n",
    "ax.scatter(points1r[0,:],points1r[1,:], c='fuchsia', marker='+')\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots() # image 2\n",
    "ax.imshow(img2c)\n",
    "ax.scatter(points2[0,:],points2[1,:], c='cyan', marker='+')\n",
    "ax.scatter(points2r[0,:],points2r[1,:], c='fuchsia', marker='+')\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Green'> - Refine the homography 23 with the Gold Standard algorithm and visualize the differences in the keypoint locations. </span>\n",
    "\n",
    "<span style='color:Green'> - Build mosaic with the refined homographies. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. OPTIONAL. Application: Calibration with a planar pattern and augmented reality**\n",
    "\n",
    "As we have seen in class, we can calibrate a camera with a planar pattern with Zhang's algorithm. A key aspect of this algorithm is the estimation of a homography that relates a pair of images, in particular, the template image and an image of it taken with the camera we want to calibrate. Once the camera is calibrated and we have recovered the relative pose between the camera and the planar pattern we can properly insert a virtual object on top of the flat pattern in a way that it is consistent, in a perspective sense, with rest of the scene.\n",
    "\n",
    "### **4.1 Camera calibration**\n",
    "\n",
    "The following code calibrates a camera using N (where N greater or equal than three) views of the planar pattern. Most of the code is provided but there are parts you need to complete. These parts are indicated in green before the corresponding cell code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read template image and calibration images\n",
    "template = cv2.imread('Data/calib/template.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "images = []\n",
    "N = 3 # 3, 4, or 5\n",
    "for i in range(1,N+1):\n",
    "    m = cv2.imread(\"Data/calib/graffiti{0}.tif\".format(i),cv2.IMREAD_GRAYSCALE)\n",
    "    images.append(m)\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "# find the keypoints and descriptors with ORB\n",
    "kpt, dest = orb.detectAndCompute(template,None)\n",
    "kpi, desi = [], []\n",
    "for m in images:\n",
    "    kp, des = orb.detectAndCompute(m,None)\n",
    "    kpi.append(kp)\n",
    "    desi.append(des)\n",
    "\n",
    "\n",
    "# Keypoint matching and homography estimation\n",
    "bf = cv2.BFMatcher()\n",
    "H = []\n",
    "for i in range(N):\n",
    "    matches = bf.knnMatch(dest,desi[i],k=2)\n",
    "    # Apply ratio test\n",
    "    good_matches = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.75*n.distance:\n",
    "            good_matches.append([m])\n",
    "    \n",
    "    # Fit homography and remove outliers\n",
    "    points1 = []\n",
    "    points2 = []\n",
    "    for m in good_matches:\n",
    "        points1.append([kpt[m[0].queryIdx].pt[0], kpt[m[0].queryIdx].pt[1], 1])\n",
    "        points2.append([kpi[i][m[0].trainIdx].pt[0], kpi[i][m[0].trainIdx].pt[1], 1])\n",
    "    \n",
    "    points1 = np.asarray(points1)\n",
    "    points1 = points1.T\n",
    "    points2 = np.asarray(points2)\n",
    "    points2 = points2.T\n",
    "    \n",
    "    Hi, indices_inlier_matches = Ransac_DLT_homography(points1, points2, 3, 1000)\n",
    "    H.append(Hi)\n",
    "    \n",
    "    # Show inlier matches\n",
    "    inlier_matches = itemgetter(*indices_inlier_matches)(good_matches)\n",
    "    img_12 = cv2.drawMatchesKnn(template,kpt,images[i],kpi[i],inlier_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.imshow(img_12)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Green'> - Complete the code that computes the Image of the Absolute Conic. </span>\n",
    "\n",
    "<span style='color:Green'> - Complete the code that computes the matrix of internak parameters, K. </span>\n",
    "\n",
    "Note: there is a linalg.cholesky function both in numpy and scipy, you have to read the documentation of both and find out which is the proper one to use here.\n",
    "\n",
    "<span style='color:Green'> - In the report, make some comments related to the obtained internal camera parameters and also comment their relation to the image size. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Image of the Absolute Conic ...\n",
    "\n",
    "# Compute the matrix of internal parameters, K ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Green'> - Complete the calculation of r1, r2, and ti in the code below. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute camera position and orientation\n",
    "import math\n",
    "R = []\n",
    "t = []\n",
    "P = []\n",
    "\n",
    "for i in range(N):\n",
    "    # compute r1, r2, and t{i}\n",
    "    h = H[i]\n",
    "    r1 = # complete ...\n",
    "    r2 = # complete ...\n",
    "    ti = # complete ...\n",
    "    \n",
    "    # Solve the scale ambiguity by forcing r1 and r2 to be unit vectors.\n",
    "    s = math.sqrt(np.linalg.norm(r1) * np.linalg.norm(r2)) * np.sign(ti[2])\n",
    "    r1 = r1 / s\n",
    "    r2 = r2 / s\n",
    "    ti = ti / s\n",
    "    t.append(ti)\n",
    "    Ri = np.array([r1, r2, np.cross(r1,r2)])\n",
    "    Ri = Ri.T\n",
    "    \n",
    "    # Ensure R is a rotation matrix\n",
    "    U, d, Vt = np.linalg.svd(Ri)\n",
    "    Ri = U @ np.identity(3) @ Vt\n",
    "    R.append(Ri)\n",
    "   \n",
    "    # Pi = K * [Ri ti]\n",
    "    A = np.zeros((3,4))\n",
    "    A[:3,:3] = Ri\n",
    "    A[:,3] = ti\n",
    "    Pi = K @ A\n",
    "    P.append(Pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code (no need to be completed) draws the planar pattern and the N camera locations (position and orientation).\n",
    "\n",
    "<span style='color:Green'> - Take a look to the provided code for that and in the report explain how the optical center is computed. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from utils import plot_camera, plot_image_origin\n",
    "\n",
    "ny, nx = images[0].shape\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(N):\n",
    "    plot_camera(P[i], nx, ny, fig, \"camera{0}\".format(i))\n",
    "\n",
    "plot_image_origin(nx, ny, fig, \"image\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can draw a fixed camera and N planar patterns at different relative poses with respect to the camera.\n",
    "\n",
    "<span style='color:Green'> -Complete the function ' plot_image_Rt' in order to achieve that. Note: it may be useful to look at function 'plot_image_origin' in utils.py</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_Rt(R,t,w, h, fig, legend):\n",
    "    p1 = # to complete ...\n",
    "    p2 = # to complete ...\n",
    "    p3 = # to complete ...\n",
    "    p4 = # to complete ...\n",
    "\n",
    "    x = np.array([p1[0], p2[0], p3[0], p4[0], p1[0]])\n",
    "    y = np.array([p1[1], p2[1], p3[1], p4[1], p1[1]])\n",
    "    z = np.array([p1[2], p2[2], p3[2], p4[2], p1[2]])\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(x=x, y=z, z=-y, mode='lines',name=legend))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "A = np.zeros((3,4))\n",
    "A[0,0]=A[1,1]=A[2,2]=1\n",
    "\n",
    "plot_camera(K@A, nx, ny, fig, \"camera\")\n",
    "for i in range(N):\n",
    "    plot_image_Rt(R[i], t[i], nx, ny, fig, \"image{0}\".format(i))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Augmented reality**\n",
    "\n",
    "The following code (no need to be completed) draws a simple 3D virtual object (a cube) on top of the flat pattern.\n",
    "\n",
    "<span style='color:Green'> - Analyse the provided code and in the report explain how the virtual object is inserted in the image and why the camera view needs to be calibrated. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Th, Tw = template.shape\n",
    "cube_corners = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1], [0, 0, 1], [0, 0, 0], [1, 0, 0], [1, 0, 1], [0, 0, 1], [0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 1, 1], [0, 1, 1], [0, 1, 0]])\n",
    "\n",
    "import numpy.matlib\n",
    "offset = np.array([Tw/2, Th/2, -Tw/8])\n",
    "M = cube_corners.shape[0]\n",
    "X = (cube_corners - 0.5) * Tw/ 4 + np.matlib.repmat(offset, M, 1)\n",
    "\n",
    "X = X.T\n",
    "ones = np.ones(M)\n",
    "Xh = np.stack((X[0,:], X[1,:], X[2,:], ones), axis=0)\n",
    "\n",
    "line_color = (0, 255, 0)\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    xp = P[i]@Xh\n",
    "    xp = xp / xp[2,:]\n",
    "    xp = xp.astype(int)\n",
    "    \n",
    "    image = cv2.imread(\"Data/calib/graffiti{0}.tif\".format(i+1),cv2.IMREAD_COLOR)\n",
    "    image_gray = image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image_gray[:, :, 0] = gray[:, :]\n",
    "    image_gray[:, :, 1] = image_gray[:, :, 0]\n",
    "    image_gray[:, :, 2] = image_gray[:, :, 0]\n",
    "    for j in range(M-1):\n",
    "        image = cv2.line(image_gray, (xp[0,j],xp[1,j]), (xp[0,j+1],xp[1,j+1]), line_color, 4) \n",
    "    plt.imshow(image)\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. OPTIONAL. Application: Logo detection**\n",
    "\n",
    "<span style='color:Green'> - Detect the UPF logo in the two UPF images using the DLT algorithm (folder \"logos\").\n",
    "Interpret and comment the results. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. OPTIONAL. Application: Logo replacement**\n",
    "\n",
    "<span style='color:Green'> - Replace the UPF logo by the master logo in one of the previous images using the DLT algorithm. </span>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
